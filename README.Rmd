---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# deduped

<!-- badges: start -->
<!-- badges: end -->

The goal of `deduped` is to provide one main utility function (`deduped`) that makes it easier to speed up functions that are commonly run on vectors with significant duplication.

One particular use case that I come across a lot is when using `basename` and `dirname` on the `file_path` column after reading multiple CSVs (e.g. with `readr::read_csv(..., id="file_path")`). `basename` and `dirname` are surprisingly slow (especially on Windows), and most of the column is duplicated.

## Installation

You can install the development version of `deduped` like so:

``` r
if(!requireNamespace("remotes")) install.packages("remotes")

remotes::install_github("orgadish/dedup")
```

## Basic Example

```{r example}
library(deduped)

x <- sample(LETTERS, 10)
x

large_x <- sample(rep(x, 100))
length(large_x)

slow_func <- function(x) for(i in x) {Sys.sleep(0.001)}

system.time({y <- slow_func(large_x)})
system.time({y2 <- deduped(slow_func)(large_x)})
all(y == y2)
```

## `file_path` Example

```{r file_path_example}

# Create multiple CSVs to read
tf <- tempfile()
dir.create(tf)

# Duplicate mtcars 10,000x and write 1 CSV for each value of `am`
large_mtcars <- dplyr::slice(mtcars, rep(1:nrow(mtcars), 10000))
invisible(sapply(
  dplyr::group_split(large_mtcars, am),
  function(x) {
    file_name <- paste0("mtcars_", unique(x$am), ".csv")
    readr::write_csv(x, file.path(tf, file_name))
  }
))

large_x <- readr::read_csv(
  list.files(tf, full.names = TRUE),
  id = "file_path"
)
large_x

system.time({y1 <- dplyr::mutate(large_x, file_name = basename(file_path))})
system.time({y2 <- dplyr::mutate(large_x, file_name = deduped(basename)(file_path))})

all.equal(y1, y2)

unlink(tf)
```
